{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Group 6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Test"
      ],
      "metadata": {
        "id": "dI4-dzzLScL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install bitarray fastBPE hydra-core omegaconf regex requests sacremoses subword_nmt sacrebleu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wEofTztmDDQ",
        "outputId": "7907ab45-7c6f-4386-fb0e-8a207ae1a8aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitarray in /usr/local/lib/python3.7/dist-packages (2.3.5)\n",
            "Requirement already satisfied: fastBPE in /usr/local/lib/python3.7/dist-packages (0.1.0)\n",
            "Requirement already satisfied: hydra-core in /usr/local/lib/python3.7/dist-packages (1.1.1)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.7/dist-packages (2.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (0.0.47)\n",
            "Requirement already satisfied: subword_nmt in /usr/local/lib/python3.7/dist-packages (0.3.8)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core) (5.4.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.7/dist-packages (from hydra-core) (4.8)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.7/dist-packages (from omegaconf) (6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.62.3)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.7/dist-packages (from subword_nmt) (4.0.3)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.4.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.19.5)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2.3.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.9)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core) (3.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# List available models\n",
        "torch.hub.list('pytorch/fairseq')  # [..., 'transformer_lm.wmt19.en', ...]"
      ],
      "metadata": {
        "id": "ywrEpc-0u9z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class Seer:\n",
        "    def __init__(self, dataset = \"roberta.base\"):\n",
        "        roberta = torch.hub.load(\"pytorch/fairseq\", dataset)\n",
        "        roberta.eval()\n",
        "        self.model = roberta\n",
        "        self.tail = [\"?\", \"!\", \"»\"]\n",
        "        self.deadtail = [\".\", \":\", \"...\", \"…\", \",\", \";\"]\n",
        "        self.end = [\"?\", \"!\", \"!)\", '?\"', \"»\", \"?]\", \"[\", \"(\", \"?'\", \"???\", \"(?\", \"('\"]\n",
        "        self.deadend = [\".\", \":\", \"...\", \"…\", \",\", \";\", \"...?\", \"]\", \")\", \"'?\", \"?)\", \"')\"]\n",
        "\n",
        "    def predict(self, text, i=0, sentence=\"\"):\n",
        "        predictions = self.model.fill_mask(text + \" <mask>\", topk=5)\n",
        "        unmasked = predictions[i][2].strip()\n",
        "        if len(sentence.split(\" \")) < 4:\n",
        "            if unmasked not in self.tail and unmasked not in self.deadtail:\n",
        "                sentence += \" \" + unmasked\n",
        "                text += \" \" + unmasked\n",
        "                return(self.predict(text, 0, sentence))\n",
        "        if len(sentence.split(\" \")) == 1:\n",
        "            return unmasked\n",
        "        if unmasked in self.tail:\n",
        "            return sentence.strip() + \" \" + unmasked\n",
        "        if unmasked in self.deadtail:\n",
        "            return sentence.strip() + unmasked\n",
        "\n",
        "    def predict_further(self, text, i=0, sentence=\"\"):\n",
        "        length = len(sentence.split(\" \"))\n",
        "        predictions = self.model.fill_mask(text + \"<mask>\", topk=10)\n",
        "        unmasked = predictions[i][2].strip()\n",
        "        if length < 4:\n",
        "            if unmasked in self.end or unmasked in self.deadend:\n",
        "                return self.predict_further(text, i + 1, sentence)\n",
        "            sentence += \" \" + unmasked\n",
        "            text += \" \" + unmasked\n",
        "            return(self.predict_further(text, 0, sentence))\n",
        "        if unmasked in self.end:\n",
        "            return sentence.strip() + \" \" + unmasked\n",
        "        if unmasked in self.deadend:\n",
        "            return sentence.strip() + unmasked\n",
        "        return sentence.strip() + \" \" + unmasked\n",
        "\n",
        "    def predict_2(self, data):\n",
        "        predict = []\n",
        "        for i in range(5):\n",
        "            text = data[\"text\"].strip()\n",
        "            result = self.predict(text, i)\n",
        "            if result not in predict:\n",
        "                predict.append(result)\n",
        "        return predict\n",
        "\n",
        "    def predict_4(self, data):\n",
        "        predict = []\n",
        "        for i in range(5):\n",
        "            text = data[\"text\"].strip()\n",
        "            try:\n",
        "                result = self.predict_further(text, i)\n",
        "                if result not in predict:\n",
        "                    predict.append(result)\n",
        "            except:\n",
        "                continue\n",
        "        return predict\n",
        "\n",
        "Nostradamus = Seer()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5ZWhcuVtpjG",
        "outputId": "4706f866-c39c-4f58-b1a5-d0612062598d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_fairseq_main\n",
            "2022-01-22 03:11:43 | INFO | fairseq.file_utils | loading archive file http://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz from cache at /root/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350\n",
            "/usr/local/lib/python3.7/dist-packages/hydra/experimental/initialize.py:36: UserWarning: hydra.experimental.initialize() is no longer experimental. Use hydra.initialize()\n",
            "  message=\"hydra.experimental.initialize() is no longer experimental.\"\n",
            "/usr/local/lib/python3.7/dist-packages/hydra/experimental/compose.py:19: UserWarning: hydra.experimental.compose() is no longer experimental. Use hydra.compose()\n",
            "  message=\"hydra.experimental.compose() is no longer experimental.\"\n",
            "/usr/local/lib/python3.7/dist-packages/hydra/core/default_element.py:126: UserWarning: In 'config': Usage of deprecated keyword in package header '# @package _group_'.\n",
            "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/changes_to_package_header for more information\n",
            "  See {url} for more information\"\"\"\n",
            "/root/.cache/torch/hub/pytorch_fairseq_main/fairseq/checkpoint_utils.py:421: UserWarning: \n",
            "'config' is validated against ConfigStore schema with the same name.\n",
            "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
            "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
            "  state = load_checkpoint_to_cpu(filename, arg_overrides)\n",
            "/usr/local/lib/python3.7/dist-packages/hydra/compose.py:54: UserWarning: \n",
            "The strict flag in the compose API is deprecated and will be removed in the next version of Hydra.\n",
            "See https://hydra.cc/docs/upgrades/0.11_to_1.0/strict_mode_flag_deprecated for more info.\n",
            "\n",
            "  \"\"\"\n",
            "/root/.cache/torch/hub/pytorch_fairseq_main/fairseq/models/roberta/model.py:376: UserWarning: \n",
            "'config' is validated against ConfigStore schema with the same name.\n",
            "This behavior is deprecated in Hydra 1.1 and will be removed in Hydra 1.2.\n",
            "See https://hydra.cc/docs/next/upgrades/1.0_to_1.1/automatic_schema_matching for migration instructions.\n",
            "  **kwargs,\n",
            "2022-01-22 03:11:45 | INFO | fairseq.tasks.masked_lm | dictionary: 50264 types\n",
            "2022-01-22 03:11:48 | INFO | fairseq.models.roberta.model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 25, 'log_format': 'json', 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': 128, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': 1.0, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 512, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': 19812, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 200, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': True, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 999999, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': 'mmap', 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 999999, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': True, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0006], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 10, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 512}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='roberta_base', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, arch='roberta_base', attention_dropout=0.1, best_checkpoint_metric='loss', bpe='gpt2', bucket_cap_mb=200, clip_norm=0.0, cpu=False, criterion='masked_lm', curriculum=0, data='/root/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350', dataset_impl='mmap', ddp_backend='c10d', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_no_spawn=False, distributed_port=19812, distributed_rank=0, distributed_world_size=512, dropout=0.1, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, ffn_blocks_to_remove=-1, ffn_reg_scale_factor=0.0, find_unused_parameters=True, fix_batches_to_gpus=False, force_anneal=None, fp16=True, fp16_init_scale=4, fp16_scale_tolerance=0.0, fp16_scale_window=128, global_sync_iter=10, keep_interval_updates=-1, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.1, load_checkpoint_heads=True, log_format='json', log_interval=25, lr=[0.0006], lr_scheduler='polynomial_decay', mask_prob=0.15, max_epoch=0, max_positions=512, max_sentences=16, max_sentences_valid=16, max_source_positions=512, max_target_positions=512, max_tokens=999999, max_update=500000, maximize_best_checkpoint_metric=False, memory_efficient_fp16=True, mha_heads_to_keep=-1, mha_reg_scale_factor=0.0, min_loss_scale=0.0001, min_params_to_wrap=100000000, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_token_positional_embeddings=False, num_workers=2, only_validate=False, optimizer='adam', optimizer_overrides='{}', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, random_token_prob=0.1, required_batch_size_multiple=1, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete', save_interval=1, save_interval_updates=2000, seed=1, sentence_avg=False, skip_invalid_size_inputs_valid_test=True, spectral_norm_classification_head=False, stop_min_lr=-1, task='masked_lm', tbmf_wrapper=False, threshold_loss_scale=1.0, tokenizer=None, tokens_per_sample=512, total_num_update=500000, train_subset='train', untie_weights_roberta=False, update_freq=[1], use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_updates=24000, weight_decay=0.01), 'task': {'_name': 'masked_lm', 'data': '/root/.cache/torch/pytorch_fairseq/37d2bc14cf6332d61ed5abeb579948e6054e46cc724c7d23426382d11a31b2d6.ae5852b4abc6bf762e0b6b30f19e741aa05562471e9eb8f4a6ae261f04f9b350', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.15, 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 1, 'include_target_tokens': False}, 'criterion': {'_name': 'masked_lm', 'tpu': True}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': True, 'lr': [0.0006]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0006]}, 'scoring': None, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "req_data = \"Michael Obama is \"\n",
        "\n",
        "content = Nostradamus.predict_2({\n",
        "    \"text\": req_data\n",
        "})\n",
        "print(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnDjatpowYUV",
        "outputId": "17e90360-c556-43ef-a133-71236b0ec7b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['dead.', 'gay.', 'president.', 'President.', 'retiring.']\n"
          ]
        }
      ]
    }
  ]
}